<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>能够复原大多数魔方的解法</title>
    <url>/2023/02/19/cube_solution/</url>
    <content><![CDATA[<p>多年前看到过一个视频，讲述了对于结构对称的魔方的通用解法。在此结合原视频和魔方吧里的帖子记录一下以作存档。</p>
<p>以3阶魔方为例，只要会复原一层，就可以用这个解法复原整个魔方。复原魔方在于将棱块和角块移到正确的位置，并且色向也正确。如果会复原一层的话，那自然可以任意地移动角块棱块的位置，改变其色向。现在可以构造一个公式，用于某个棱块朝向的翻转。翻转后顶层的其他棱角不受影响，下面两层被打乱了。把翻转公式逆着做一遍，魔方又可以复原到原来的状态。
<img src="/images/魔方.png" height="500" width="500" /> <span id="more"></span>
假如我在翻转完棱块后，将顶层转90度，再逆做翻转公式，那么最后我的顶层将会有两个棱块色向翻转了，而下面二层是复原的。因此，我就可以构造一个翻棱公式，这个公式可以同时将两个棱块的色向改变。同理，我也可以构造出翻角公式，一次性将两个角块的色向翻转。构造完换色向的公式后，也可以构造出换位置的公式，构造一个公式，同时交换两个棱块或者角块的位置。将这个公式做两遍，可以达到3轮换的效果。</p>
<p>于是，四个公式便可以完成3阶魔方的复原，即棱块转向、角块转向、棱块位置轮换、色块位置轮换。</p>
<p>上述复原的基础在于结构对称的魔方可以构造出一个正向操作，和对应的逆向操作，使得做完正向再做逆向后，魔方可以恢复到初始转态。用一个公式来描述就是<code>ABA'B'</code>。若魔方经过该公式后状态不变，则称A操作和B操作遵循交律。大多数魔方公式可以基于这个模式进行构造。</p>
<p>采用这个方法复原魔方，若最后角块位置正确（不管色向），但出现了棱块的2轮换，那么再做一次2轮换，将棱块状态变为3轮换，进行复原。若棱块位置正确，角块出现2轮换也是同理。</p>
]]></content>
      <categories>
        <category>兴趣爱好</category>
      </categories>
      <tags>
        <tag>魔方</tag>
      </tags>
  </entry>
  <entry>
    <title>博客美化</title>
    <url>/2023/02/09/hexo%E7%BE%8E%E5%8C%96/</url>
    <content><![CDATA[<h1 id="next主题配置文件修改">Next主题配置文件修改</h1>
<p>参考<a
href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/DATA-FILES.md">官方的修改意见</a>，不要直接修改next原始配置文件，避免更新主题时文件冲突。本博客新建了<code>source/_data/next.yml</code>，并将next.yml的override字段改为true，之后所有修改都在该文件进行，修改会覆盖原始配置文件。</p>
<h1 id="图片插入">图片插入</h1>
<p>不需要下载hexo-asset-image,将hexo<code>_config.yml</code>配置文件中post_asset_folder的值改为true。输入指令<code>hexo new "My New Post"</code>，新建md文件及同名文件夹，将图片放入该文件夹内。文章中插入<code>&#123;% asset_img example.jpg %&#125;</code>,即可正常显示图片。
<span id="more"></span>
或者将post_asset_folder改为false，在source目录下新建images文件夹，图片放在这个文件夹内。<code>![](/images/test.png)</code>引用图片，也可以用HTML标签<code>&lt;img src="/images/test.png" height="500" width="500" /&gt;</code>。</p>
<h1 id="统计网站运行时间字数访客">统计网站运行时间,字数,访客</h1>
<h2 id="运行时间">运行时间</h2>
<p>修改<code>\themes\next\layout\_partials\footer.swig</code>,添加以下代码：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!--《添加网站运行时间 --&gt;</span><br><span class="line">&lt;!--&lt;br/&gt;--&gt;</span><br><span class="line">&lt;span id=&quot;timeDate&quot;&gt;载入天数...&lt;/span&gt;&lt;span id=&quot;times&quot;&gt;载入时分秒...&lt;/span&gt;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">    var now = new Date();</span><br><span class="line"></span><br><span class="line">    function createtime() &#123;</span><br><span class="line">        var grt = new Date(&quot;02/08/2023 18:42:36&quot;); //此处修改你的建站时间或者网站上线时间 </span><br><span class="line">        now.setTime(now.getTime() + 250);</span><br><span class="line">        days = (now - grt) / 1000 / 60 / 60 / 24;</span><br><span class="line">        dnum = Math.floor(days);</span><br><span class="line">        hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum);</span><br><span class="line">        hnum = Math.floor(hours);</span><br><span class="line">        if (String(hnum).length == 1) &#123;</span><br><span class="line">            hnum = &quot;0&quot; + hnum;</span><br><span class="line">        &#125;</span><br><span class="line">        minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum);</span><br><span class="line">        mnum = Math.floor(minutes);</span><br><span class="line">        if (String(mnum).length == 1) &#123;</span><br><span class="line">            mnum = &quot;0&quot; + mnum;</span><br><span class="line">        &#125;</span><br><span class="line">        seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);</span><br><span class="line">        snum = Math.round(seconds);</span><br><span class="line">        if (String(snum).length == 1) &#123;</span><br><span class="line">            snum = &quot;0&quot; + snum;</span><br><span class="line">        &#125;</span><br><span class="line">        document.getElementById(&quot;timeDate&quot;).innerHTML = &quot; | 本站已安全运行 &quot; + dnum + &quot; 天 &quot;;</span><br><span class="line">        document.getElementById(&quot;times&quot;).innerHTML = hnum + &quot; 小时 &quot; + mnum + &quot; 分 &quot; + snum + &quot; 秒&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    setInterval(&quot;createtime()&quot;, 250);</span><br><span class="line">&lt;/script&gt;</span><br><span class="line">&lt;!-- 添加网站运行时间》 --&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="字数">字数</h2>
<p>安装插件<code>npm install hexo-symbols-count-time --save</code>,修改hexo<code>_config.yml</code>配置文件中以下代码：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  symbols: true                # 文章字数统计</span><br><span class="line">  time: true                   # 文章阅读时长</span><br><span class="line">  total_symbols: true          # 站点总字数统计</span><br><span class="line">  total_time: true             # 站点总阅读时长</span><br><span class="line">  exclude_codeblock: true     # 排除代码字数统计</span><br></pre></td></tr></table></figure>
修改主题配置（即source/_data/next.yml）文件<code>_config.yml</code>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">symbols_count_time:</span><br><span class="line">  separated_meta: true    # 是否另起一行（true的话不和发表时间等同一行）</span><br><span class="line">  item_text_post: true    # 首页文章统计数量前是否显示文字描述（本文字数、阅读时长）</span><br><span class="line">  item_text_total: true   # 页面底部统计数量前是否显示文字描述（站点总字数、站点阅读时长</span><br></pre></td></tr></table></figure></p>
<h2 id="访客">访客</h2>
<p>打开主题配置文件即themes_config.yml，找到busuanzi_count，改为true
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">busuanzi_count:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure> 打开_partials.swig，添加如下内容： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% if theme.busuanzi_count.enable %&#125;</span><br><span class="line">    &lt;script async src=&quot;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">    &lt;span id=&quot;busuanzi_container_site_pv&quot;&gt;总访问量&lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt;次&lt;/span&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">    &lt;span id=&quot;busuanzi_container_site_uv&quot;&gt;总访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人&lt;/span&gt;</span><br><span class="line">    &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;</span><br><span class="line">&lt;!-- 不蒜子计数初始值纠正 --&gt;</span><br><span class="line">&lt;script&gt;</span><br><span class="line">$(document).ready(function() &#123;</span><br><span class="line"></span><br><span class="line">    var int = setInterval(fixCount, 50);  // 50ms周期检测函数</span><br><span class="line">    var countOffset = 0;  // 初始化首次数据</span><br><span class="line"></span><br><span class="line">    function fixCount() &#123;            </span><br><span class="line">       if (document.getElementById(&quot;busuanzi_container_site_pv&quot;).style.display != &quot;none&quot;)</span><br><span class="line">        &#123;</span><br><span class="line">            $(&quot;#busuanzi_value_site_pv&quot;).html(parseInt($(&quot;#busuanzi_value_site_pv&quot;).html()) + countOffset); </span><br><span class="line">            clearInterval(int);</span><br><span class="line">        &#125;                  </span><br><span class="line">        if ($(&quot;#busuanzi_container_site_pv&quot;).css(&quot;display&quot;) != &quot;none&quot;)</span><br><span class="line">        &#123;</span><br><span class="line">            $(&quot;#busuanzi_value_site_uv&quot;).html(parseInt($(&quot;#busuanzi_value_site_uv&quot;).html()) + countOffset); // 加上初始数据 </span><br><span class="line">            clearInterval(int); // 停止检测</span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;</span><br><span class="line">       	</span><br><span class="line">&#125;);</span><br><span class="line">&lt;/script&gt; </span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>
删除themes_third-party-counter.swig文件的<code>&lt;span class="post-meta-divider"&gt;|&lt;/span&gt;</code>,否则访客统计下会多出一条竖线。</p>
<h1 id="设置网站背景图片">设置网站背景图片</h1>
<p>打开主题配置文件即themes_config.yml，将
<code>style: source/_data/styles.styl</code> 取消注释： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">  style: source/_data/styles.styl</span><br></pre></td></tr></table></figure>
打开根目录Blog_data.styl，添加以下内容： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">body &#123;</span><br><span class="line">      background: url(../images/background.png);//自己喜欢的图片地址</span><br><span class="line">      background-size: cover;</span><br><span class="line">      background-repeat: no-repeat;</span><br><span class="line">      background-attachment: fixed;</span><br><span class="line">      background-position: 50% 50%;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
将图片改名为background.png，保存到。</p>
<h1 id="评论留言">评论留言</h1>
<p>本博客使用了基于<a
href="https://www.leancloud.cn/">LeanCould</a>的Valine评论系统，首先在LeanCloud官网注册账号并创建应用。修改next主题配置文件中Valine部分代码,enable改为true，复制appid和appkey填入
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">valine:</span><br><span class="line">  enable: false</span><br><span class="line">  appid: # Your leancloud application appid</span><br><span class="line">  appkey: # Your leancloud application appkey</span><br><span class="line">  notify: false # Mail notifier</span><br><span class="line">  verify: false # Verification code</span><br><span class="line">  placeholder: Just go go # Comment box placeholder</span><br><span class="line">  avatar: mm # Gravatar style</span><br><span class="line">  guest_info: nick,mail,link # Custom comment header</span><br><span class="line">  pageSize: 10 # Pagination size</span><br><span class="line">  language: # Language, available values: en, zh-cn</span><br><span class="line">  visitor: false # Article reading statistic</span><br><span class="line">  comment_count: true # If false, comment count will only be displayed in post page, not in home page</span><br><span class="line">  recordIP: false # Whether to record the commenter IP</span><br><span class="line">  serverURLs: # When the custom domain name is enabled, fill it in here (it will be detected automatically by default, no need to fill in)</span><br><span class="line">  #post_meta_order: 0</span><br></pre></td></tr></table></figure>
hexo默认在所有页面开启评论，可通过修改md文件中<code>comments: false</code>属性值关闭或开启评论。</p>
<h1 id="设置博文内链接为蓝色">设置博文内链接为蓝色</h1>
<p>打开themes/next/source/css/_common/components/post/post.styl文件，将下面的代码复制到文件最后：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.post-body p a&#123;</span><br><span class="line">    color: #0593d3;</span><br><span class="line">    border-bottom: none;</span><br><span class="line">    &amp;:hover &#123;</span><br><span class="line">      color: #0477ab;</span><br><span class="line">      text-decoration: underline;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="渲染公式">渲染公式</h1>
<p>参考<a
href="https://theme-next.js.org/docs/third-party-services/math-equations.html#mjx-eqn%3Aeq1">官方方式</a>。首先卸载hexo-renderer-marked，然后安装hexo-renderer-pandoc。hexo-renderer-pandoc依赖于pandoc,所以还需要<a
href="https://github.com/jgm/pandoc/blob/main/INSTALL.md">安装pandoc</a>,然后将其加入环境变量。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm un hexo-renderer-marked</span><br><span class="line">npm i hexo-renderer-pandoc</span><br></pre></td></tr></table></figure>
如果hexo-renderer-pandoc的版本大于等于5.0.0，则需要在hexo的配置文件_config.yml中加入
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pandoc:</span><br><span class="line">  args:</span><br><span class="line">    - --mathjax</span><br></pre></td></tr></table></figure>
最后修改主题配置文件，即next.yml中的math部分，如果next主题版本高于v6.3.0，tags设置为ams，可以启用公式自动编号的特性。详见<a
href="https://docs.mathjax.org/en/latest/input/tex/eqnumbers.html">Automatic
Equation Numbering特性</a>。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">  # That is it only render those page which has `mathjax: true` in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: false</span><br><span class="line"></span><br><span class="line">  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br><span class="line">    # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">    # mhchem: false</span><br><span class="line">    tags: ams</span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux常用指令</title>
    <url>/2023/03/09/tmux%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="启动与退出">启动与退出</h1>
<p>输入<code>tmux</code>，进入tmux,按<code>Ctlr + d</code>或者输入<code>exit</code>，退出tmux窗口。退出后该对话将不存在。</p>
<h1 id="查看tmux窗口历史输出">查看tmux窗口历史输出</h1>
<p>先进入tmux会话，然后按<code>Ctlr + b</code>，接着按<code>[</code>，即可进入查看模式，方向键滚动显示。按<code>q</code>退出查看模式。</p>
<h1 id="会话管理">会话管理</h1>
<h2 id="新建会话">新建会话</h2>
<p>第一个启动的 Tmux
窗口，编号是0，第二个窗口的编号是1，以此类推。以下指令可新建会话并为会话起名。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tmux new -s &lt;session-name&gt;</span><br></pre></td></tr></table></figure> <span id="more"></span> ## 分离会话 在 Tmux
窗口中，按下<code>Ctrl+b d</code>或者输入<code>tmux detach</code>命令，就会将当前会话与窗口分离。命令执行后，就会退出当前
Tmux 窗口，但是会话和里面的进程仍然在后台运行。
<code>tmux ls</code>命令可以查看当前所有的 Tmux 会话。</p>
<h2 id="接入会话">接入会话</h2>
<p><code>tmux attach</code>命令用于重新接入某个已存在的会话。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用会话编号</span><br><span class="line">tmux attach -t 0</span><br><span class="line"></span><br><span class="line">使用会话名称</span><br><span class="line">tmux attach -t &lt;session-name&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="杀死会话">杀死会话</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用会话编号</span><br><span class="line">tmux kill-session -t 0</span><br><span class="line"></span><br><span class="line">使用会话名称</span><br><span class="line">tmux kill-session -t &lt;session-name&gt;</span><br></pre></td></tr></table></figure>
<h2 id="切换会话">切换会话</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">使用会话编号</span><br><span class="line">tmux switch -t 0</span><br><span class="line"></span><br><span class="line">使用会话名称</span><br><span class="line">$ tmux switch -t &lt;session-name&gt;</span><br></pre></td></tr></table></figure>
<h2 id="重命名会话">重命名会话</h2>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">将0号会话重命名</span><br><span class="line">tmux rename-session -t 0 &lt;new-name&gt;</span><br></pre></td></tr></table></figure>
<h1 id="参考链接">参考链接</h1>
<p>https://www.ruanyifeng.com/blog/2019/10/tmux.html</p>
]]></content>
      <categories>
        <category>指令</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>tmux</tag>
      </tags>
  </entry>
  <entry>
    <title>东北之旅</title>
    <url>/2023/07/28/%E4%B8%9C%E5%8C%97%E4%B9%8B%E6%97%85/</url>
    <content><![CDATA[前几天去东北找同学玩了一趟，体验了东北搓澡和烧烤。在沈阳的几天去了大帅府，在商业区逛了逛，还是挺繁华的。不过某些区域也比较破败了。总体生活成本比南方低多了。
<span id="more"></span>
<div>
<p><img src="\images\东北\IMG_6287.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6290.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6291.jpg" style="max-width: 70%; max-height: 70%;"></p>
</div>
从沈阳出发先去了延吉，吃了冷面，逛了下朝鲜民俗风情园。这个园子完全被拍照的女士们占领了，延吉那么热的天，她们还穿长袖裙子拍照，真是佩服。摄影师一个个也都是黢黑黢黑的，没少晒。
<div>
<p><img src="\images\东北\yanji.png" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6298.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6299.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6301.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6302.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6304.jpg" style="max-width: 70%; max-height: 70%;"></p>
</div>
延吉吃了午饭和晚饭后就动身前往长白山，山下一个小镇的居民，都把自己家改造成了旅店饭店。第二天上长白山，运气非常好，遇上了晴天，看到了天池，在这前几天和几天后，都是有雨的日子。美中不足的是没有赶上上长白山的第一班大巴，这导致了我们北坡有个景点（谷底森林）没看，其余景点除了天池，看的比较急。
<div>
<p><img src="\images\东北\IMG_6321.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6322.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6335.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6336.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6341.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6342.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6349.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6351.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6352.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6355.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6360.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6363.jpg" style="max-width: 70%; max-height: 70%;"></p>
</div>
<p>当天下了长白山就坐火车去大连了，中途在长春换乘，这是比较痛苦的时候。白天爬了山，晚上凌晨2点的火车，在火车站按摩椅上躺了会。晚上9:30左右到的长春，我们在市区吃了饭，顺便走了走，真是非常破旧，完全不像市区的样子，仿佛还在上个世纪。</p>
我们在大连待了两天，这两天主要吃了不少东西，喜鼎的海胆水饺真是非常不错，很香。还尝试了海星，很难吃，绿色的看着也没有食欲，吃了几口就扔了，晚饭尝试了生腌，生腌螃蟹挺好吃，其他一些海螺鲍鱼比较一般，还吃了凉拌象拔蚌。大连感觉比沈阳啊，长春要新，更加热闹繁华。第二天去了旅顺博物馆，晚上吃了海鲜，吃的真爽，我们两个人点了3到4人套餐，最后没吃完，实在是太多了。另外可能是这些天饮食不规律，没休息好，再加上吃的太多了。我搞的上吐下泻，无福享受更多美食，不过好在旅行也进行了尾声。
<div>
<p><img src="\images\东北\IMG_6388.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6389.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6396.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6401.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6402.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6404.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6408.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6409.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6410.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6415.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6416.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6417.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6418.jpg" style="max-width: 70%; max-height: 70%;">
<img src="\images\东北\IMG_6419.jpg" style="max-width: 70%; max-height: 70%;"></p>
</div>
<p>最后一天回到了沈阳，看了长安三万里，真是很棒，特别是李白将进酒那一段，画面意象俱佳。由于肠胃问题，中午去吃了潮汕牛肉火锅，还不错。晚上去按摩，结束后发现开店的只有火锅烧烤了，于是不得不去吃了烧烤，按理是不应该吃这样的。</p>
<p>在东北的这一周体验很不错，吃到了很多美食，东北菜就是量大管饱非常适合我哈哈。这次从北坡上的长白山，基本全程坐车，此外还有南坡和西坡两条路线，以后要是有机会再去。大连海鲜不错，比较便宜，不过住宿挺贵。</p>
]]></content>
      <categories>
        <category>旅行</category>
      </categories>
      <tags>
        <tag>大连</tag>
        <tag>沈阳</tag>
        <tag>延吉</tag>
        <tag>长白山</tag>
      </tags>
  </entry>
  <entry>
    <title>博物馆--钢琴博物馆</title>
    <url>/2023/02/26/%E5%8D%9A%E7%89%A9%E9%A6%86-%E9%92%A2%E7%90%B4%E5%8D%9A%E7%89%A9%E9%A6%86/</url>
    <content><![CDATA[<p>打算记录整理一下以前去过的有特色的一些博物馆。一直以来都很喜欢去博物馆，每每看到藏品，我都不禁会想这件藏品后有着怎么的故事，藏品曾经的主人有什么样的人生。一件物品跨越时空呈现在我眼前，让我觉得很不可思议，想想这中间几百上千年要经历多少事。历经岁月后它被我看到，透过它我觉得我与千百年前的人、时代产生了联系，这对我来说就像奇迹一般，我被这种沧桑感，宏大感所吸引。</p>
<p>去过的大多中国博物馆，大都千篇一律，从建筑上看基本都是一个风格，展品上来说也都是遵循时间循序进行陈列，从石器时代一直到近现代。感觉缺乏特色没有新意，除了馆里的一些珍品外，可以说博物馆间几乎没有区分度了。我将陆续写写去过的一些与众不同的博物馆，可能以后也会补上一些常规的博物馆，以后再说吧，比较有特色的一定要记录一下。第一篇是重庆的钢琴博物馆，据说是亚洲最大的钢琴博物馆。重庆还有一个小提琴博物馆，可惜当时一直没去，希望以后有机会的话去看看。
<span id="more"></span>
馆内收藏了各个时代的钢琴，可以看出从钢琴制作起始到现在的演变。也有一些作曲家的手稿。</p>
<p>这是鲁宾斯坦演奏过的钢琴，19世纪的琴和现在的已经很像了，比起现在的琴少了一个踏板。
<img src="\images\piano\Rubinstein.jpg"/></p>
<p>这是李斯特演奏过的钢琴，能感觉到音箱的形状和琴键布局和现在不太一样。琴键数看起来没有88个。
<img src="\images\piano\李斯特演奏1.jpg"/>
<img src="\images\piano\李斯特演奏.jpg"/></p>
<p>这是贝多芬演奏过的。 <img src="\images\piano\贝多芬演奏.jpg"/></p>
<p>这是舒曼定制的钢琴，保存状态很好，看起来还蛮新的。
<img src="\images\piano\舒曼定制.jpg"/></p>
<p>馆里还有一个中国风的钢琴，我记得似乎是为某位中国小姐定制的。
<img src="\images\piano\中国风.jpg"/></p>
<p>这是一台连体钢琴，可以两个人同时演奏，在这之前不晓得还有这种钢琴，好想听听一起演奏是什么效果。
<img src="\images\piano\双子星.jpg"/></p>
<p>这台琴带了几个旋钮还是拉栓，看起来有点管风琴那味哈哈哈。
<img src="\images\piano\带旋钮.jpg"/></p>
<p>这是一台长的很高的钢琴，不知道为啥这样设计。
<img src="\images\piano\长得很高.jpg"/></p>
<p>这个钢琴博物馆里头钢琴很多，大厅和走廊都是琴。
<img src="\images\piano\大厅.jpg"/>
<img src="\images\piano\走廊.jpg"/></p>
<p>舒伯特的手稿，魔王的最初版本，与最后出版的不太一样。
<img src="\images\piano\舒伯特手稿.jpg"/></p>
]]></content>
      <categories>
        <category>博物馆</category>
      </categories>
      <tags>
        <tag>钢琴</tag>
        <tag>古典音乐</tag>
      </tags>
  </entry>
  <entry>
    <title>澳门10公里</title>
    <url>/2023/03/20/%E6%BE%B3%E9%97%A810%E5%85%AC%E9%87%8C/</url>
    <content><![CDATA[昨天参加了澳门金沙的10公里比赛，这是我第一次正儿八经地跑10公里，阳康后训练了一个多月，感觉跑的不错，比较轻松地完成了比赛。一些老爷爷老太太真有活力，也很热情。比赛途中我就看到了那位拿假蛋挞的女士，当时好想问她那个能不能吃，笑死。
<span id="more"></span>
<div>
<p><img src="\images\macau_10k\1.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\macau_10k\2.jpg" style="max-width: 50%; max-height: 50%;"/>
<img src="\images\macau_10k\3.jpg" style="max-width: 50%; max-height: 50%;"/>
<img src="\images\macau_10k\4.jpg" style="max-width: 50%; max-height: 50%;"/>
<img src="\images\macau_10k\6.jpg" style="max-width: 50%; max-height: 50%;"/>
<img src="\images\macau_10k\5.jpg" style="max-width: 50%; max-height: 50%;"/></p>
</div>
]]></content>
      <categories>
        <category>运动</category>
      </categories>
      <tags>
        <tag>澳门</tag>
        <tag>跑步</tag>
      </tags>
  </entry>
  <entry>
    <title>走进大模型(一)</title>
    <url>/2025/06/18/%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B(%E4%B8%80)/</url>
    <content><![CDATA[本文将以nano-gpt为例，讲讲大模型的整个推理过程。帮助初学者快速了解大模型，对其有一个宏观认识。本文的讲解内容大部分来源于该网站<a
href="https://bbycroft.net/llm">LLM
Visualization</a>，该网站将大模型做了可视化。 <span id="more"></span>
<div>
<p><img src="\images\llm1\20250530205048.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>上面的图片展示从输入到输出的整个流程，中间LLM部分即为nano-gpt的简易结构图。</p>
<h2 id="tokenization-and-embedding">1.Tokenization And Embedding</h2>
<p>首先，模型的输入是一个句子，然而模型是无法直接处理字符的，它只是处理数字类型，所以要先将文字转换为数字，文字到数字的这个过程叫做tokenization。那么这个过程是如何实现的呢？答案很简单，首先将输入的句子进行分词，完整的句子被分为多块，分块后的文字叫做token。通过查表的方法，看每个token对应的数字是多少。这个表键为文字（也就是token），值为数字。因此，输入一个句子，查表后，可以得到句子所对应的一串数字。在中文情况下，一个token可能是一个字，也可能是若干个字；英文场景下，一个token可能是一个单词，也可能只是某个单词一部分。</p>
下面的图展示了英文和中文进行分词后的结果，以及转换成的数字。相同颜色的区域是同一个块，可以看到sophisticated被分成了3块，是3个token。我来自中国这句话也是3个token，”来自“被算为一个token。token对应的数字称为token索引，
<div>
<p><img src="\images\llm1\20250530213103.png" style="max-width: 50%; max-height: 50%;">
<img src="\images\llm1\20250530213116.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
由token索引，从token Embed向量取token index对应的列向量，从position
Embed向量中，取出token在整个输入中位置索引对应的列向量，将两个向量相加，得到该token对应的Input
Embed。token Embed的shape为(C， n_vocab)，position Embed的shape为(C，
T)。C是向量维度或者称为特征数，通道数，T是token数量，同时也代表了时间，例如索引0代表0时刻，索引1代表1时刻。n_vocab是词汇表token数量。
<div>
<p><img src="\images\llm1\20250604211338.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
如上图所示，CBABBC是输入，每个单词代表一个token，第4个字母B，其token索引为1，所以取token
Embed中第一列，B本身所在输入中索引为3，所以取position
Embed中的第4列向量，两个向量相加，得到token B对应的Input Embed。token
Embed和position
Embed通过训练获得。对输入的每个token做同样的操作，得到完整的Input
Embed，shape为（C， T）。
<div>
<p><img src="\images\llm1\20250604212158.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<h2 id="layernorm">2.LayerNorm</h2>
<p>得到Input
Embed后，会进行normalization操作，将其变为均值为0，标准差为1的向量。首先按列求出输入每一列的均值和标准差，分别记为μ和α。</p>
对Input
Embed中每列的每个元素减去均值μ，除以标准差α。标准差根号下加ε，防止分母为0。再乘β，加γ。得到Layer
Norm。β和γl均由模型训练得到，Layer Norm会作为self-attention模块的输入。
<div>
<p><img src="\images\llm1\20250604222614.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<h2 id="self-attention">3.Self Attention</h2>
self-attenrion有多个注意力头，下面以一个头为例，每个头计算一样。在这一阶段，输入为上一阶段的Layer
Norm,我们要用q,k,v权重及其对应的bias,计算得到对应的q,k,v向量。q,k,v权重及其对应的bias均为训练获得。Q/K/V_Weights的shape为（A，
C），C即token的维度，A为token维度 / 注意力头数。
<div>
<p><img src="\images\llm1\20250607092019.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
Q_Weights矩阵与Layer
Norm矩阵相乘，再与Q_Bias相加，得到Q_vectors。K_vectors和V_vectors计算同理。三个矩阵shape为（A，
T）。
<div>
<p><img src="\images\llm1\动画.gif" style="max-width: 50%; max-height: 50%;"></p>
</div>
下面，将Q_vectors的列向量与K_vectors的列向量进行点乘（就是Q矩阵乘K矩阵的转置），点乘后，除以<span
class="math inline">\(\sqrt{A}\)</span>,A为Q/K/V_vectors的维度。除以维度的开方，是为了softmax数值稳定，避免有极大值。结果储存在Attention
Matrix中。Q_vectors的第t列与K_vectors中的所有列点乘后，得到Attention
Matrix的t这一行。Attention Matrix shape为（T， T）。Attention
Matrix的第t行，代表了第t个query分别与每个key的相似度，点乘结果越大，表示相似度越高。
<div>
<p><img src="\images\llm1\attention.gif" style="max-width: 50%; max-height: 50%;"></p>
</div>
接着对Attention Matrix每一行进行softmax运算，得到Attn Matrix
Softmax，其每一行累加和为1。
<div>
<p><img src="\images\llm1\20250607132532.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
Attn Matrix
Softmax的第t行中的每个元素，与V_vectors的每一列相乘，再累加。得到V_Output的第t列。重复上述计算，得到完整的V_Output，shape为（A，
T）。
<div>
<p><img src="\images\llm1\v_output.gif" style="max-width: 50%; max-height: 50%;"></p>
</div>
每个注意力头都会计算得到一个V_Output，我们将这些V_Output按行拼接，得到一个shape（C，
T）的矩阵。前面提到，A等于token的维度除以主力头数，所以将每个注意力头的output拼接后，维度又恢复为C。Projection
Weights与拼接后的V_Output相乘再加Projection Bias,得到Attention Output。
<div>
<p><img src="\images\llm1\20250607141607.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>Attention Ouptut会与前面得到Input Embed相加，得到Attention
Residual矩阵，shape为（C， T）。</p>
<h2 id="mlp">4.MLP</h2>
上一步得到的Attention
Residual，会和第2小节一样，进行LayerNorm。接着，MLP
Weights与LayerNorm矩阵相乘，再加MLP Bias，然后GELU激活函数激活，得到MLP
Activation矩阵。MLP Weights shape为（C*4， C）。
<div>
<p><img src="\images\llm1\20250607143845.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
然后，MLP Projection Weights与转置后的MLP Activation矩阵相乘，再加MLP
Projection Bias，得到MLP Result，shape恢复为（C， T）。
<div>
<p><img src="\images\llm1\20250607144210.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>MLP Result会和Attention Residual相加，得到MLP Residual。</p>
<h2 id="finalouput">5.FinalOuput</h2>
MLP Residual同样会进行LayerNorm运算。
<div>
<p><img src="\images\llm1\20250607145628.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
LM Head Weights（shape为（C，
n_vocab））与LayerNorm矩阵相乘，得到Logits矩阵，shape为（T，
n_vocab）。Logits按列进行softmax计算，得到Logits Softmax，shape为（T，
n_vocab）。从Logits
Softmax中，我们可以得知每个时刻，各个token出现的概率。不同的采样算法决定了模型的输出，例如可以直接选择概率最大的token，也可以根据概率分布随机抽取token。此外，还可以通过temperature参数来控制概率分布情况，在得到Logits后，将其除以temperature，再进行softmax计算。可以想到，temperature越高，那么概率分布会越平均，最后的输出随机性越强。
<div>
<p><img src="\images\llm1\20250607150053.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
]]></content>
      <categories>
        <category>llm技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>nano-gpt</tag>
        <tag>llm</tag>
      </tags>
  </entry>
  <entry>
    <title>走进大模型（二）</title>
    <url>/2025/06/29/%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B(%E4%BA%8C)/</url>
    <content><![CDATA[<p>上一篇讲了nano-gpt的结构，这篇讲讲nano-gpt的代码实现，项目来源于<a
href="https://github.com/karpathy/nanoGPT">karpathy/nanoGPT: The
simplest, fastest repository for training/finetuning medium-sized
GPTs.</a>。这个项目结构不复杂，作者也给了详细的注释，非常适合新手入门。
<span id="more"></span> ## 1. quick start
将项目clone到本地后，只需4条指令，即可实现nano-gpt的训练和推理。
首先安装项目依赖： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install torch numpy transformers datasets tiktoken wandb tqdm</span><br></pre></td></tr></table></figure>
然后运行prepare.py文件，生成训练数据(train.bin)和测试数据（val.bin），同时还会生成meta.pkl文件，帮助对数据编码与解码。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python data/shakespeare_char/prepare.py</span><br></pre></td></tr></table></figure> 接下来就是训练： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python train.py config/train_shakespeare_char.py</span><br></pre></td></tr></table></figure> 最后进行推理：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python sample.py --out_dir=out-shakespeare-char</span><br></pre></td></tr></table></figure> 下面的章节依次讲讲prepare.py， train.py，
sample.py三个文件。prepare.py在项目中有多个，在data目录不同的文件夹下，实现思路和目的都是类似的，只是针对不同的数据集，处理有些不一样，本文提到的prepare.py指data/shakespeare_char路径下的。</p>
<h2 id="数据准备">2. 数据准备</h2>
来看prepare.py文件，实现很简单，首先下载莎士比亚的诗歌，然后输出字母数量。
<div>
<p><img src="\images\llm2\20250620210102.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
统计诗歌中出现过多少种字符，并打印。然后编写字符的编码和接码函数，一个字符对应一个数字，同理，一个数字对应一个字符，两者一一对应。encode其实就是将输入token化的过程，只不过这里是每个字符作为一个token。
<div>
<p><img src="\images\llm2\20250620210804.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
往下是划分训练集和测试集，训练集默认90%，测试集10%，并保存划分好的数据集。
<div>
<p><img src="\images\llm2\20250620210944.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
最后保存meta
data,meta.pkl由三部分组分，一个是词表大小，一个是数字到字符的映射，一个是字符到数字的映射。
<div>
<p><img src="\images\llm2\20250620211016.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>到这里，prepare.py就结束了。</p>
<h2 id="训练">3.训练</h2>
train.py脚本支持分布式训练，由于作者是单卡，所以代码中涉及分布式的部分就略过。代码开头定义了一些超参和训练相关参数。若命令行输入了配置文件<code>config/train_shakespeare_char.py</code>，则<code>exec(open('configurator.py').read())</code>语句会用<code>config/train_shakespeare_char.py</code>中定义的变量值覆盖train中的同名全局变量值。没有命令行没有输入，则使用train中的定义的值。代码中的ddp变量标识是否使用分布式训练。
<div>
<p><img src="\images\llm2\file-20250620223144684.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<div>
<p><img src="\images\llm2\file-20250620223205951.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<div>
<p><img src="\images\llm2\file-20250623224131156.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p><code>train_shakespeare_char.py</code>文件包含了训练相关参数，代码内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># train a miniature character-level shakespeare model</span><br><span class="line"># good for debugging and playing on macbooks and such</span><br><span class="line">out_dir = &#x27;out-shakespeare-char&#x27;</span><br><span class="line">eval_interval = 250 # keep frequent because we&#x27;ll overfit</span><br><span class="line">eval_iters = 200</span><br><span class="line">log_interval = 10 # don&#x27;t print too too often</span><br><span class="line"></span><br><span class="line"># we expect to overfit on this small dataset, so only save when val improves</span><br><span class="line"></span><br><span class="line">always_save_checkpoint = False</span><br><span class="line">wandb_log = False # override via command line if you like</span><br><span class="line">wandb_project = &#x27;shakespeare-char&#x27;</span><br><span class="line">wandb_run_name = &#x27;mini-gpt&#x27;</span><br><span class="line">dataset = &#x27;shakespeare_char&#x27;</span><br><span class="line">gradient_accumulation_steps = 1</span><br><span class="line">batch_size = 64</span><br><span class="line">block_size = 256 # context of up to 256 previous characters</span><br><span class="line"></span><br><span class="line"># baby GPT model :)</span><br><span class="line"></span><br><span class="line">n_layer = 6</span><br><span class="line">n_head = 6</span><br><span class="line">n_embd = 384</span><br><span class="line">dropout = 0.2</span><br><span class="line">learning_rate = 1e-3 # with baby networks can afford to go a bit higher</span><br><span class="line"></span><br><span class="line">max_iters = 5000</span><br><span class="line"></span><br><span class="line">lr_decay_iters = 5000 # make equal to max_iters usually</span><br><span class="line"></span><br><span class="line">min_lr = 1e-4 # learning_rate / 10 usually</span><br><span class="line"></span><br><span class="line">beta2 = 0.99 # make a bit bigger because number of tokens per iter is small</span><br><span class="line"></span><br><span class="line">warmup_iters = 100 # not super necessary potentially</span><br><span class="line"># on macbook also add</span><br><span class="line"># device = &#x27;cpu&#x27;  # run on cpu only</span><br><span class="line">compile = False # do not torch compile the model</span><br></pre></td></tr></table></figure>
往下定义了get_batch函数，用于获取数据，进行训练，x是input，y是label，block_size控制句子长度。label内容即为x中每个token的下一位token。例如完整句子token为1
2 3 4. 那么x可能为1 2 3, y就为2 3 4。
<div>
<p><img src="\images\llm2\file-20250623224257069.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
接着初始化迭代轮次和val_loss，读取meta,pkl输出数据的基本信息，meta仅在此处使用，后续代码没有用到。
<div>
<p><img src="\images\llm2\file-20250623224949982.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
然后进行模型加载和初始化，训练支持加载已有权重继续训练，也可以从头训练。
<div>
<p><img src="\images\llm2\file-20250623225300706.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
下面定义了梯度缩放，主要解决float16训练时梯度下溢问题。定义了optimizer，是否使用torch2.0的新特性，编译模型，以及是否将模型复制到多个GPU上分布式训练模型。
<div>
<p><img src="\images\llm2\file-20250623225647993.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
随后是两个函数，一个是用于计算loss，一个是更新学习率。
<div>
<p><img src="\images\llm2\file-20250624210406160.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
下面是训练时log和训练前的初始化。mfu（model flops
utilization）衡量计算效率，model.py中定义了<code>estimate_mfu</code>函数。
<div>
<p><img src="\images\llm2\file-20250624210457443.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
以下就是训练的主体代码，训练过程中开启wanbd的话，会记录下模型的loss,lr,mfu,iter等信息，并且训练中保存val
loss最低的模型。随后就是前向推理，反向传播更新梯度。最后计算mfu。到此，整个train.py就结束了。
<div>
<p><img src="\images\llm2\file-20250624210958717.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<div>
<p><img src="\images\llm2\file-20250624211017256.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
下面看模型的前向传播，首先根据输入idx,得到token embedding和position
embedding。将两个向量相加，并应用dropout正则化，随机将某些维度置零。随后x进入循环，依次进入每个block。每一个block其实就是四个部分，分别是LayerNorm，CausalSelfAttention，LayerNorm，以及MLP。遍历计算完所有block，计算logits,求交叉熵损失。返回logits和loss。
<div>
<p><img src="\images\llm2\file-20250624212008467.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<h2 id="推理">4.推理</h2>
推理部分比较简单，首先设置推理时的相关参数，读取模型。
<div>
<p><img src="\images\llm2\file-20250625190728962.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
然后读取meta.pkl，对输入进行编码，对输出解码。最后调用generate函数进行推理。
<div>
<p><img src="\images\llm2\file-20250625191001759.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
以下是generate的定义，max_new_tokens表示依次可生成的最大token数，self.config.block_size控制上下文长度，idx_cond进行模型，进行推理，输出logits。logits除以温度，然后取top_k个token，接着对logits进行softmax计算，得到概率。最后对probs进行随机采样，得到下一个token，并将下一个token加到输入尾部，再次循环进行以上操作，直到达到设置的最大token数（max_new_tokens）。
<div>
<p><img src="\images\llm2\file-20250625191145138.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
]]></content>
      <categories>
        <category>llm技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>nano-gpt</tag>
        <tag>llm</tag>
      </tags>
  </entry>
  <entry>
    <title>走进大模型(三)——attention</title>
    <url>/2025/07/05/%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B(%E4%B8%89)/</url>
    <content><![CDATA[<p>这篇主要讲attention机制诞生的背景，为了解决什么问题而被发明，以及如何理解self-attention的计算。</p>
<h1 id="序列模型">1.序列模型</h1>
<p>在attention发明以前，对自然语言进行处理往往采用序列模型，如RNN，LSTM等。序列模型有两个问题，一个是信息丢失，一个是遗忘问题，也可以称为长距离依赖问题。信息丢失体现为编码器会将输入序列压缩为一个固定长度的向量，这个向量再传递给解码器进行解码，可想而知，当输入承载的信息超过向量能够承载的信息时，编码时就会丢失信息。第二个遗忘问题，RNN和LSTM是马尔可夫链式的结构，当前状态依赖前一步的状态，当前状态无法直接和前一步之前的信息进行交互。反向传播时，梯度从最后一个状态反向传播到第一个状态，梯度反传涉及连乘，随着输入长度的增加，很容易发生梯度爆炸或梯度消失，因此早期时的信息在传递过程中会逐渐衰减，导致模型难以捕捉远距离的依赖关系。
<span id="more"></span></p>
<h1 id="attention的发明">2.attention的发明</h1>
attention机制最早出现在Bahdanau等人的论文<a
href="https://arxiv.org/pdf/1409.0473.pdf">“Neural machine translation
by jointly learning to align and
translate.”</a>中，用于翻译任务。输入记为x，长度为n，输出记为y，长度为m。
<div>
<p><img src="\images\llm3\file-20250702223554461.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
编码器为一个双向RNN，计算输入的前向隐藏状态和反向隐藏状态，最后将前向和反向的状态拼接，作为位置i的隐藏状态。
<div>
<p><img src="\images\llm3\file-20250702223651256.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
解码器同样也会使用RNN计算输出的隐藏状态<span
class="math inline">\(s_t\)</span>，作为输出位置t的隐藏状态。同时，还会计算<span
class="math inline">\(c_t\)</span>和<span
class="math inline">\(α_{\text{t,i}}\)</span>，<span
class="math inline">\(c_t\)</span>是context vector，<span
class="math inline">\(α_{\text{t,i}}\)</span>是输出位置t与输入位置i之间的alignment
score。两者计算公式如下：
<div>
<p><img src="\images\llm3\file-20250704184946455.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>score(<span class="math inline">\(s_{\text{t-1}}\)</span>, <span
class="math inline">\(h_i\)</span>)由一个一层的前馈神经网络算出。Bahdanau等人的注意力机制通过计算context
vector，解决了信息丢失问题，可以较好地表示输入信息。传统序列模型将输入序列编码为固定长度向量，而Bahdanau等人对编码器所有隐藏状态进行加权求和，动态生成context
vector。同时alignment
score引入了对齐机制，提升了翻译任务的准确度。然而，因为结构上还存在RNN，所以遗忘问题仍然存在。
从Bahdanau的注意力诞生到self-attention的出现，中间还出现了其他注意力机制，这里略过。</p>
<h1 id="self-attetion">3.self-attetion</h1>
<p>2017年，大名鼎鼎的self-attention（<a
href="https://arxiv.org/abs/1706.03762">[1706.03762] Attention Is All
You
Need</a>）出现了。谷歌的人员提出了一种名为transformer的网络架构，其中self-attention对输入经过线性变换生成Q(query)，K(key)，V(value)三个矩阵，用这三个矩阵来计算注意力矩阵，<span
class="math inline">\(d_k\)</span>是矩阵k的维度，为公式如下：</p>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(
\frac{QK^\top}{\sqrt{d_k}} \right) V
\]</span></p>
<p>为增强模型的表达能力，论文使用多个头进行注意力计算，计算完后再将多个头的结果拼接。如何理解这三个矩阵的运算呢，这样相乘的意义是什么，我看了3Blue1Brown的视频，觉得讲得很好，有兴趣的可以观看原视频（<a
href="https://www.bilibili.com/video/BV1TZ421j7Ke/?spm_id_from=333.1387.upload.video_card.click&amp;vd_source=2f63917659c87b2e9377cefd1e9fc1c2">【官方双语】直观解释注意力机制，Transformer的核心
|
【深度学习第6章】_哔哩哔哩_bilibili</a>）。本小节使用的图片均来自于该视频。</p>
transformer首先将输入转换为向量，这些向量在高维空间中指向的方向代表了各自的语义，如图所示：
<div>
<p><img src="\images\llm3\file-20250705085812458.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
在这个高维空间中，词与词之间具有什么样的关系呢？假设当前有4个向量，分别是aunt,uncle,woman,man。在这个高维空间中，woman向量减去man向量得到的黄色向量，约等于aunt向量减去uncle向量。可以看到，向量在高维空间中经过线性变换，可以得到具有其他语义的向量。注意力就是这么一个通过线性变换，结合上下文来更新向量的机制，通过self-attention，每个向量在高维空间中会指向尽可能精确代表它自身语义的方向。
<div>
<p><img src="\images\llm3\file-20250705090015020.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
下面来看self-attention机制是如何实现在高维空间中更新向量的。输入向量记为x,shape为（C，
T），C为向量维度，T为token数量。x与q_weights相乘再加bias得到Q矩阵，Q
shape为（A， T），A为token维度 /
注意力头数，T同样代表token数量。x与k_weights相乘再加bias得到K矩阵,K
shape也为（A，
T）。Q可以视为对当前token的查询，Q想从整个输入中找到其他token与当前token的关系，以一个具体的例子来看：
<div>
<p><img src="\images\llm3\file-20250705132437529.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
creature位置的token，想通过Q查询前面有没有形容词，用来形容creature，该句子中也就是fluffy和blue。这里所说的查询前面是否有形容词只是一方面，实际上，模型想要查询的信息可能不止这些。x通过q_weights进行线性变换，该查询被编码为向量（针对单个token来看），针对每个token的查询，组成了Q矩阵。K则可以视为对Q的回复，x通过k_weights进行线性变换，fluffy和blue对应的向量会被映射到与creature向量高度一致的方向上，方向一致意味着相关度高，更加匹配。
<div>
<p><img src="\images\llm3\file-20250705133518218.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<span
class="math inline">\(\frac{QK^\top}{\sqrt{d_k}}\)</span>得到注意力矩阵，shape为（T，
T）。这个矩阵由Q的列向量与K的列向量点乘算得。点乘结果越大，表示相似度越高，图中以圆圈大小表示相似度，圆越大，表示点乘结果越大。
<div>
<p><img src="\images\llm3\file-20250705133914351.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
注意，transformer推理和训练是逐token进行的，每预测一个token，新token加入输出序列，然后根据输出序列又预测下一个token。为了不让序列中的未来信息泄露，即后面的词不能影响前面的词，注意力矩阵的上三角部分会被填充为负无穷（以行的视角来看）。如果以列视角来看，就是下三角部分被填充为负无穷。因为注意力矩阵shape是（T，
T），所以从哪个视角来看都行。填充后，对注意力矩阵按行进行softmax，使得每行总和为1。
<div>
<p><img src="\images\llm3\file-20250705135014770.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
前面提到，self-attention机制要做到在高维空间中更新向量，使其指向最能代表其语义的方向。将注意力矩阵与V矩阵相乘，我们可以达成这个目标。V矩阵由x通过v_weights线性变换得到。shape为（A，
T），代表输入token的语义。橙色向量代表creature，是V中的一个列向量，经过self-attention计算后，它会变为蓝色向量（蓝色向量语义为fluffy
blue creature），creature向量的方向精确指向了它在该句中（the fluffy blue
creature roamed the verdant forest）的含义，它是如何变换为蓝色向量的呢？
<div>
<p><img src="\images\llm3\file-20250705140427696.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
答案就是注意力矩阵与V矩阵相乘，结果相当于V矩阵的加权和，权重体现了输入中每个token与该token的相关度。在下图中，是以列视角出发，对注意力矩阵按列进行softmax。对V矩阵的每一列求加权和（如果以行视角看，权重为注意力矩阵的行向量，以列视角看，权重为注意力矩阵中的列向量），得到self-attention的最终输出，这个输出再与原始的输入向量相加（残差结构）。每一个token在高维空间中都由了精确表示，下图中就是creature最终指向了fluffy
blue creature的方向。
<div>
<p><img src="\images\llm3\file-20250705142045129.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
上面所说的是单头注意力的计算，transformer是多头注意力，每一个注意力头都会进行上述的计算。最后将每个注意力头的结果按行拼接。在上面的例子中，我们关注的是形容词（fluffy，blue）如何影响名词（creature），关注点聚焦于语法层面。然而，上下文影响词义的方式是多样的，具体例子可以看下图：
<div>
<p><img src="\images\llm3\file-20250705143957973.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
这个例子中车的结构发生了改变。
<div>
<p><img src="\images\llm3\file-20250705144158659.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>这个例子中，根据上下文不同，Harry代表的含义也不相同，一个表示哈利波特，一个表示哈利王子。多头注意力正是为了捕获不同的注意力模式，实际上，我觉得注意力头捕获的模式非常抽象，难以解释和描述，上面几个具体的例子只是为了说明上下文可以有不同的关注模式。模型的每个注意力头实际上捕获的模式可能是杂糅的，人类无法理解的。</p>
<p>到这，做一个总结，self-attention的发明解决了信息丢失和长序列的遗忘问题，通过注意力矩阵显示地对每一个token根据上下文计算注意力分数，或称为权重。多头注意力可以学习不同的表示模式，概念。此外，多个注意力头可以并行训练（序列模型因为当前状态依赖前一个状态，因此无法并行训练），使得计算效率提高，模型规模扩大较为容易，使后面的大语言模型出现成为可能。</p>
]]></content>
      <categories>
        <category>llm技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>llm</tag>
        <tag>attention</tag>
        <tag>序列模型</tag>
      </tags>
  </entry>
  <entry>
    <title>澳门路环徒步</title>
    <url>/2023/05/17/%E6%BE%B3%E9%97%A8%E8%B7%AF%E7%8E%AF%E5%BE%92%E6%AD%A5/</url>
    <content><![CDATA[<p>昨天天气不错，临时起意不去学校了。决定去路环逛逛，一直没有去过，最想去的就是龙爪角，看到别人发，觉得风景不错。</p>
到了路环，先去安德鲁蛋挞那买了两个蛋挞。路环这个店还是很原始的样子，没有过多装换，没啥游客，几乎都是本地人在购买，店里非常朴素，做蛋挞蛋糕的地方都是开放的，没有隔档。
<span id="more"></span>
<div>
<p><img src="\images\路环徒步\IMG_5627.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
安德鲁蛋挞对面是卫生站，一个粉色的房子，看起来很温馨。
<div>
<p><img src="\images\路环徒步\IMG_5629.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
从卫生站往左边走，可以走到码头，非常近，靠海的那一侧房子都是用瓦楞搭起来的，不少人家往海的方向对房子进行了扩展，在陆地上的只有很小一部分。码头附近还有几家卖海鲜干货的。午餐选了一家海景不错的，也是比较大的一家店。
<div>
<p><img src="\images\路环徒步\IMG_5636.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5638.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
吃完饭走到十月初五马路，发现这边吃的更多哈哈哈，有一家人特别多，后来搜了路环美食，发现这家叫雅憩，做葡国菜的，中午我从那路过，看外面的桌子都坐满了。十月初五马路很多房子彩色的，很适合拍照。
<div>
<p><img src="\images\路环徒步\IMG_5645.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5647.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
沿着十月初五马路一直走就走到山上去了，有些路段没有人行道，就只能走马路，不过车辆不多，山上是警察的射击训练营。走在这个山路上，都感觉不到这是澳门，脱离了往日繁华热闹的印象。
<div>
<p><img src="\images\路环徒步\IMG_5651.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>走到竹湾海滩那边，可以在海里游泳，沙滩上有人躺着晒太阳，澳门冬泳协会也在这，广东这带的冬天那么短，感觉都游不了几天冬泳。从十月初五马路上山到乡村马路后，沿途都有巴士站，不想走路还可以坐巴士，蛮方便的。美中不足的就是有些马路没有人行道，还是不太安全。</p>
到了龙爪角，有很多人翻出护栏钓鱼。这个海滨步道依托原始的地形修建，几乎没有太多改动，就加了一些石梯，铺筑了路面，在比较平整的地方就没有人工改动，直接走在山体的石头上。有些路段还有山上的泉水留下。大概中途的位置有一个观海亭，风还挺大的。
<div>
<p><img src="\images\路环徒步\IMG_5658.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5663.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5678.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5682.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
从龙爪角出来就是黑沙海滩，之后我就前往石排湾郊野公园，走的山上的步道，上去的时候倒是挺累人的，好在山不是很高，在山里几乎都是平路。山里岔路很多，很容易走错方向。到了公园，有个地形图，发现这个路环这些山都是连着的。公园养了一些动物，熊猫馆里人挺多，两只熊猫活动空间很大。
<div>
<p><img src="\images\路环徒步\IMG_5690.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5697.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5698.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5700.jpg" style="max-width: 50%; max-height: 50%;">
<img src="\images\路环徒步\IMG_5704.jpg" style="max-width: 50%; max-height: 50%;"></p>
</div>
]]></content>
      <categories>
        <category>运动</category>
      </categories>
      <tags>
        <tag>澳门</tag>
        <tag>徒步</tag>
      </tags>
  </entry>
  <entry>
    <title>跑步训练与提高</title>
    <url>/2023/02/19/%E8%B7%91%E6%AD%A5%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8F%90%E9%AB%98/</url>
    <content><![CDATA[<h1 id="力量训练">力量训练</h1>
<h2 id="核心">核心</h2>
<h3 id="基础核心训练">基础核心训练</h3>
<h4 id="核心力量训练">核心力量训练</h4>
<p>上腹肌（即六块腹肌中靠上的部分）：卷腹。
下腹肌：仰卧举腿、仰卧踩单车。
躯干侧前方（腹内外斜肌）：侧卷腹、卷腹摸脚踝、曲腿刮雨器。
躯干两侧（腰方肌：控制骨盆）：侧卧起、曲腿侧卧起、侧卧并腿。
躯干后方肌肉（竖脊肌：保持躯干挺直，防止腰痛）：仰卧挺身。 <span id="more"></span>
#### 核心稳定性训练（锻炼深层肌肉）
俯桥（平板支撑）、侧桥、仰桥（臀桥）。</p>
<h3 id="专项核心训练">专项核心训练</h3>
<p>平板支撑体位交替曲腿、俯桥提膝后摆腿、侧桥位单侧模拟跑步、单腿臀桥、臀桥接提膝。完成上述动作时应注意保持核心稳定。</p>
<h2 id="臀部和大腿">臀部和大腿</h2>
<h3 id="中级提高力量">中级（提高力量）</h3>
<p>弓箭步、单腿上训练凳、保加尼亚剪蹲、单腿硬拉、单腿臀桥、弓箭步跳（爆发力训练）。</p>
<h3
id="高级增加神经肌肉协调性进一步强化关键肌肉">高级（增加神经肌肉协调性、进一步强化关键肌肉）</h3>
<p>单腿上训练凳接高抬腿、平板支撑体位交替曲腿、侧桥位单侧模拟跑步、单腿硬拉接提膝、弓箭步接高抬腿跳起（爆发力训练）。</p>
<h2
id="臀中肌保持跑步稳定的关键肌肉保持骨盆和膝关节稳定">臀中肌（保持跑步稳定的关键肌肉、保持骨盆和膝关节稳定）</h2>
<p>髋部提拉、单腿外摆、单腿浅蹲后外摆、侧卧腿外摆、侧卧贝壳式（下落时双腿膝盖不相碰）、跪姿侧桥接上摆腿、俯卧四点支撑腿外摆。练习时注意骨盆稳定，不要翻转。</p>
<h2 id="小腿提供缓冲和支撑">小腿（提供缓冲和支撑）</h2>
<h3 id="力量练习">力量练习</h3>
<p>勾脚尖练习（前侧肌肉）、提踵（单腿站立练习）。</p>
<h3 id="稳定性练习提高脚踝稳定性">稳定性练习（提高脚踝稳定性）</h3>
<p>（闭眼）抱胸单腿站立。</p>
<h3 id="缓冲练习">缓冲练习</h3>
<p>单腿下落缓冲（前脚掌落地、落地声越轻越好）。</p>
<h3 id="爆发力训练">爆发力训练</h3>
<p>单脚原地跳（落地后迅速跳起，缩短触地时间）、双脚前后跳（脚跟尽量不要触地，减少触地时间）、单脚前后跳。</p>
<h2 id="深层小肌肉让动作更加精准">深层小肌肉（让动作更加精准）</h2>
<p>所有动作都用训练带辅助。贝壳式（臀中肌）、臀桥（臀大肌、臀中肌）、下蹲（臀大肌、臀中肌）、横向跨步（臀中肌）、前后跨步（训练膝盖正对脚尖的能力，减少膝盖内扣）、单腿后外展（臀中肌）、侧卧直腿抬高（要点是上腿的后撤，臀中肌）、单腿提膝（髂腰肌）。</p>
<h1
id="跑法训练训练强度811一切以心率为准">跑法训练（训练强度8:1:1，一切以心率为准）</h1>
<p>轻松跑占8，中等和高强度跑各占1，以跑步里程计算。一个简单测算心率的方法<code>最大心率 = 220 - 年龄</code>，想更精确地知道自己的最大运动心率需要在实际运动中测量。</p>
<h2 id="lsd">LSD</h2>
<p>增强心脏功能、提高肌肉利用氧气能力、让身体适应脂肪供能。LSD控制在1小时左右，心率区间应为最大心率的65%
~
78%。若以半马为目标，则2小时左右，全马控制在最长2.5小时。在LSD跑末加几组冲刺跑保持肌肉快速收缩能力和无氧系统素质。冲刺跑可重复多组，一周3
~ 6次，冲刺与间歇时间比1:2 ~ 3。 <img src="\images\run\LSD1.png" />
<img src="\images\run\LSD2.png" /></p>
<h2 id="抗乳酸跑">抗乳酸跑</h2>
<p>提升身体耐受乳酸和排出乳酸的能力、提升有氧耐力空间（使得更不容易达到乳酸阀的拐点）、让身体在临界速度下维持更长时间，心率区间应为最大心率的88%
~
90%。进行该训练时要把握好速度，太快会很快力竭，太慢会处于有氧区间。每周2
~
3次，一次跑量不超过周跑量的10%，一般累计奔跑20分钟，训练休息时间比为5:1。
<img src="\images\run\乳酸跑.png" /></p>
<h2 id="间歇跑">间歇跑</h2>
<p>提高最大摄氧量，增强心肺功能、提升跑步经济性、提升机体抗乳酸能力，心率区间应为最大心率的91%
~ 100%。每组训练时间在3 ~ 5分钟，训练间歇时间比1:1，每次跑控制在20 ~
30分钟。 <img src="\images\run\间歇跑.png" /></p>
<h2 id="maf180">MAF180</h2>
<p>训练强度低，适合减肥和LSD训练。</p>
<h2 id="亚索800成为精英的必备训练">亚索800（成为精英的必备训练）</h2>
<p>提升最大摄氧量、提升身体耐受乳酸和排出乳酸的能力。一组800米，心率保持在最大心率的95%
~ 100%，每组时间3 ~ 4分钟。训练间歇时间比1:1，一般训练4 ~
8组，精英可提高到10组。若强度不够则将每组距离增加。</p>
<h2 id="法特莱克跑">法特莱克跑</h2>
<p>就是变速跑，可消解跑步的单调乏味。</p>
]]></content>
      <categories>
        <category>运动</category>
      </categories>
      <tags>
        <tag>跑步</tag>
        <tag>腿部训练</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2025/07/07/test/</url>
    <content><![CDATA[<p>self-attention的计算耗时，同时也需要大量空间，假设输入长度为N，那么空间复杂度和时间复杂度都是<span
class="math inline">\(N^2\)</span>，所以早期模型的上下文长度一般很有限。而flash-attention的优化，则能把原始attention的空间复杂度缩小为N。本文将细致地讲解flash-attention的实现，做到通俗易懂，对于性能优化零基础的人也能看懂。flash-attention论文链接：<a
href="https://arxiv.org/abs/2205.14135">[2205.14135] FlashAttention:
Fast and Memory-Efficient Exact Attention with IO-Awareness</a>。 #
1.总览
现代计算机基于冯诺依曼架构，如图所示，分为五部分：输入设备、输出设备、存储单元、控制单元、运算单元。
![[file-20250708212909164.png]]
完成计算需要先从储存中读出数据，调用计算核心进行计算，再将计算结果写回存储。所以在做性能优化时。往往从两方面考虑，一个是读写数据角度，一个是计算角度。有的计算可能在计算方面耗时多，有的计算可能在读写时耗时，也有的两者兼有。通过屋脊模型，可以量化地衡量某个计算是compute-bound还是memory-bound，关于屋脊模型的讲解，后续我会写一篇文章。对于flash-attention来讲，在数据读写方面很耗时，所以作者的核心思想就是通过减少数据的读写次数来加速计算。具体表现为两点：1.将矩阵分块，在SRAM（Static
Random-Access Memory）中完成计算，再写回HBM（High Bandwidth
Memory）。2.不存储中间结果，在反向传播时，通过重新计算中间结果来更新梯度。从下图（来源于论文）可以看到，flash-attention相比普通的attention，速度快了不少。
![[file-20250708224435562.png]] # 2.原始attention
我们先来看下原始的attention计算过程，Q，K，V都存在HBM中。三者shape都为（N，
d）N是输入序列的长度，d是注意力头的维度，一般来说，N远大于d，GPT2中，N为2014，d为64。第一步，从HBM中加载Q，K，计算得到注意力矩阵S，把S写回HBM，再从HBM中读入S，求softmax，得到P，再把P写回HBM，最后从HBM中加载P，V，计算得到最终结果O，写回HBM。
![[file-20250708221951972.png]]
HBM相比SRAM来说，存储容量大，读写速度慢。对于储存来说，读写速度一般与其容量呈反比，受限于成本，读写速度快的，容量就小。因此数据反复从HBM读写很耗时。下图反映了不同储存在读写速度和容量方面的差异。
![[file-20250708223339056.png]] # 3.flash-attention
flash-attention的优化分为两个方面，一个是分块计算，一个是更新梯度时重计算中间结果，重新计算中间结果的目的是为了不保存空间复杂度为<span
class="math inline">\(N^2\)</span>的中间结果，减少IO时间。 ## 3.1 Tiling
我们先看论文里作者给出的flash-attention的分块计算过程，分别对Q，K，V进行分块后，从HBM读出数据到SRAM，计算完成后，在把数据写入HBM。依次对每一个分块进行处理，直到计算完所有分块。
![[file-20250708224029546.png]]
论文给出了以上计算过程的详细算法，如下所示，我会对每一步进行讲解。
![[file-20250708224719815.png]] 第一步，计算分块的行数<span
class="math inline">\(B_c\)</span>和列数<span
class="math inline">\(B_r\)</span>，M是SRAM的大小，<span
class="math inline">\(B_c\)</span>等于<span
class="math inline">\(\frac{M}{4\mathrm{d}}\)</span>，除4d是要保证SRAM能够装下4个分块，分别是Q，K，V，以及最终结果O。<span
class="math inline">\(B_r\)</span>在<span
class="math inline">\(\frac{M}{4\mathrm{d}}\)</span>和d之间取最小值是为了保证SRAM能够装下中间计算用到的所有数据，除了QKVO的分块，还有<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>，<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>等。</p>
<p>第二步会初始化O，<span
class="math inline">\({\ell}\)</span>，m，分别为0，0，负无穷。<span
class="math inline">\({\ell}\)</span>，m是在计算softmax过程中用到的两个统计值，分别代表局部的指数和以及局部最大值。我们回顾一下softmax的计算方法，下图中x为输入向量，<span
class="math inline">\(m(x)\)</span>为x所有值中的最大值，<span
class="math inline">\(f(x)\)</span>中的每一项，指数部分减去<span
class="math inline">\(m(x)\)</span>，是为了保持计算时数值稳定。<span
class="math inline">\({\ell(x)}\)</span>是<span
class="math inline">\(f(x)\)</span>的累加和，即softmax的分母。
![[file-20250712090217340.png | 公式一]]
由原始的softmax计算可知，得到一个向量的softmax值，需要完整的向量，因为分母求和会用到向量的每一个元素，并且分子指数部分减去最大值，最大值也需要遍历完整向量才能获取。论文对QK分块后，是怎么计算softmax的呢？我们看下图：
![[file-20250712101937966.png|公式二]] 假设向量x被分为了2块，分别为<span
class="math inline">\(x^\text{(1)}\)</span>，<span
class="math inline">\(x^\text{(2)}\)</span>，首先根据公式一计算<span
class="math inline">\(m(x^\text{(1)})\)</span>，<span
class="math inline">\(f(x^\text{(1)})\)</span>和<span
class="math inline">\({\ell(x^\text{(1)})}\)</span>，接下来读取<span
class="math inline">\(x^\text{(2)}\)</span>，计算<span
class="math inline">\(m(x)\)</span>，即<span
class="math inline">\(x^\text{(1)}\)</span>，<span
class="math inline">\(x^\text{(2)}\)</span>的最大值，<span
class="math inline">\(m(x)\)</span>就是向量x的最大值，接着就是求softmax的分子部分——<span
class="math inline">\(f(x)\)</span>，<span
class="math inline">\(e^{m(x^{(1)}) -
m(x)}f(x^{(1)})\)</span>怎么理解，其实就是原始softmax的分子部分，向量<span
class="math inline">\(x^{(1)}\)</span>的每个元素减去向量x中的最大值。把<span
class="math inline">\(f(x^{(1)}\)</span>展开后，等于<span
class="math inline">\([e^{x_1 - m(x^{(1)})}···e^{x_B -
m(x^{(1)})}]\)</span>。<span class="math inline">\(e^{m(x^{(1)}) -
m(x)}\)</span>和<span
class="math inline">\(f(x^{(1)}\)</span>相乘，指数部分的<span
class="math inline">\(m(x^{(1)})\)</span>就没了。<span
class="math inline">\(e^{m(x^{(2)}) -
m(x)}f(x^{(2)})\)</span>也是同理，我们就得到了计算softmax所需的分子。分母部分<span
class="math inline">\(e^{m(x^{(1)}) -
m(x)}\ell(x^{(1)})\)</span>也是一样的，把<span
class="math inline">\(\ell(x^{(1)})\)</span>展开，指数部分的<span
class="math inline">\(m(x^{(1)})\)</span>就被消除了。由此得到分母<span
class="math inline">\(\ell(x)\)</span>，从而计算出向量x的softmax。分块计算的结果完全等价于对完整的x向量进行softmax计算。</p>
<p>下面以一个具体的例子来看公式二的计算，公式二是理解flash-attention的关键。算法的第十一步和第十二步都是基于公式二进行。
我们有两个块： <strong>Block1</strong>:<br />
<span class="math inline">\(x^{(1)} = [2.0,\ 3.0]\)</span>
<strong>Block2</strong>:<br />
<span class="math inline">\(x^{(2)} = [0.0,\ 4.0]\)</span>
<strong>拼接后的向量为</strong>：<br />
<span class="math inline">\(x = [2.0,\ 3.0,\ 0.0,\ 4.0]\)</span>
<strong>块内局部最大值</strong> <span class="math display">\[
\begin{aligned}
m^{(1)} &amp;= \max(2.0,\ 3.0) = 3.0 \\
m^{(2)} &amp;= \max(0.0,\ 4.0) = 4.0 \\
m(x) &amp;= \max(m^{(1)},\ m^{(2)}) = \max(3.0,\ 4.0) = 4.0
\end{aligned}
\]</span><strong>各块的 <span class="math inline">\(f(x^{(i)})\)</span>
和 <span class="math inline">\(\ell(x^{(i)})\)</span></strong> 对于
Block1： <span class="math display">\[
\begin{aligned}
f(x^{(1)}) = \exp(x^{(1)} - m^{(1)}) &amp;= [e^{2.0 - 3.0},\ e^{3.0 -
3.0}] = [e^{-1},\ e^0] = [0.3679,\ 1] \\
\ell(x^{(1)}) &amp;= 0.3679 + 1 = 1.3679 \\
f(x^{(\text{block}_1)}) &amp;= e^{m^{(1)} - m(x)} \cdot f(x^{(1)}) \\
&amp;= e^{m^{(1)} - m(x)} \cdot \exp(x^{(1)} - m^{(1)}) \\
&amp;= e^{-1} \cdot [0.3679,\ 1] = [0.1353,\ 0.3679]
\end{aligned}
\]</span></p>
<p>对于 Block2： <span class="math display">\[
\begin{aligned}
f(x^{(2)}) = \exp(x^{(2)} - m^{(2)}) &amp;= [e^{0.0 - 4.0},\ e^{4.0 -
4.0}] = [e^{-4},\ 1] = [0.0183,\ 1] \\
\ell(x^{(2)}) &amp;= 0.0183 + 1 = 1.0183 \\
f(x^{(\text{block}_2)}) &amp;= e^{m^{(2)} - m(x)} \cdot f(x^{(2)}) \\
&amp;= e^{m^{(2)} - m(x)} \cdot \exp(x^{(2)} - m^{(2)}) \\
&amp;= e^0 \cdot [0.0183,\ 1] = [0.0183,\ 1]
\end{aligned}
\]</span> <strong>合并 <span class="math inline">\(\ell(x)\)</span> 和
<span class="math inline">\(f(x)\)</span></strong> <span
class="math display">\[
\begin{aligned}
\ell(x) &amp;= e^{m^{(1)} - m(x)} \cdot \ell(x^{(1)}) + e^{m^{(2)} -
m(x)} \cdot \ell(x^{(2)}) \\
&amp;= e^{-1} \cdot 1.3679 + e^0 \cdot 1.0183 \\
&amp;= 0.3679 \cdot 1.3679 + 1.0183 = 0.5034 + 1.0183 = 1.5217
\end{aligned}
\]</span></p>
<p><span class="math display">\[
f(x) = \begin{bmatrix}
f(x^{(block_1)}) \\
f(x^{(block_2)})
\end{bmatrix}
= [0.1353,\ 0.3679,\ 0.0183,\ 1.0]
\]</span></p>
<p><strong>Softmax输出</strong></p>
<p>我们使用上面得到的 <span class="math inline">\(\ell(x)\)</span> 和
<span class="math inline">\(f(x)\)</span> 计算softmax：</p>
<p><span class="math display">\[
\text{softmax}(x) = \frac{f(x)}{\ell(x)}
\]</span></p>
<p><span class="math display">\[
\text{softmax}(x) \approx [0.0889,\ 0.2417,\ 0.0120,\ 0.6569]
\]</span></p>
<p>第三步把Q按行分为<span class="math inline">\(T_r\)</span>个块，<span
class="math inline">\(T_r\)</span>的计算方法在图中已给出，每个块shape为（<span
class="math inline">\(B_r\)</span>， d），K和V也是按行分块，分为<span
class="math inline">\(T_c\)</span>个块，<span
class="math inline">\(T_c\)</span>的计算同样在图中已给出，每个块shape为（<span
class="math inline">\(B_c\)</span>， d）。第四步也是分块，O，<span
class="math inline">\({\ell}\)</span>，m，都分为<span
class="math inline">\(T_r\)</span>个块。</p>
<p>第五步开始就是计算的主体部分，第五第六步按列将分块的K和V从HBM加载到SRAM。第七第八步将分块的Q，O以及两个统计值<span
class="math inline">\(\ell\)</span>和m按行从HBM加载到SRAM。</p>
<p>第九步，在SRAM中计算注意力矩阵——<span
class="math inline">\(S_{ij}\)</span>，<span
class="math inline">\(S_{ij}\)</span>shape为（<span
class="math inline">\(B_r\)</span>，<span
class="math inline">\(B_c\)</span>）。</p>
<p>第十步，在SRAM中计算<span
class="math inline">\(S_{ij}\)</span>每一行的最大值<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，softmax的分子<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>，softmax的分母<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>。
![[file-20250712110454677.png]]</p>
<p>第十一步，也是在SRAM中计算，在下方附上公式方便查阅。这一步是计算根据读入的分块，计算新的最大值<span
class="math inline">\(m_{i}^{new}\)</span>，新的指数和<span
class="math inline">\(\ell_{i}^{new}\)</span>。进行第一次计算时，<span
class="math inline">\(m_i\)</span>是负无穷（初始化时的值），所以<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，因为<span
class="math inline">\(\ell_i\)</span>初始化为0，所以<span
class="math inline">\(\ell_{i}^{new}\)</span>计算公式的第一部分为0，第二部分指数部分为0（因为<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>），<span
class="math inline">\(\ell_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>。往后每一次的更新逻辑，其实和上文提到的公式二是一样的，<span
class="math inline">\(m_{i}^{new}\)</span>是从开头到当前块的最大值，<span
class="math inline">\(\ell_{i}^{new}\)</span>是从开头到当前块的指数和。
![[file-20250712105924118.png]]</p>
<p>第十二步，在SRAM中计算出<span
class="math inline">\(O_i\)</span>后，把<span
class="math inline">\(O_i\)</span>写回HBM，下方附上公式。<span
class="math inline">\(O_i\)</span>是<span
class="math inline">\(\text{softmax}\left( QK^\top \right)
V\)</span>。<span
class="math inline">\(diag{(\ell_{i}^{new})}^{-1}\)</span>表示将<span
class="math inline">\(\ell_{i}^{new}\)</span>向量转换为对角矩阵（只有对角线位置有值，其余位置为0），指数-1表示取倒数。<span
class="math inline">\(diag{(\ell_{i}^{new})}^{-1}\)</span>其实就是做除法。看第一部分红框，第一次计算时，因为<span
class="math inline">\(\ell_i\)</span>
为0，所以红框部分为0，绿框部分<span
class="math inline">\(e^{\tilde{m}_{\text{i,j}} -
m_{i}^{new}}\tilde{p}_{\text{i,j}}V_j\)</span>，第一次计算时<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，所以只剩<span
class="math inline">\(\tilde{p}_{\text{i,j}}V_j\)</span>，<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>是softmax的分子部分。乘<span
class="math inline">\(V_j\)</span>，除<span
class="math inline">\(\ell_{i}^{new}\)</span>，得到<span
class="math inline">\(O_i\)</span>的部分结果。第二次计算时，红框部分乘<span
class="math inline">\(diag(\ell_i)\)</span>是为了抵消第一次除<span
class="math inline">\(\ell_{i}^{new}\)</span>，因为在每次计算都是在最外层除<span
class="math inline">\(\ell_{i}^{new}\)</span>，为了避免多除，所以要乘<span
class="math inline">\(diag(\ell_i)\)</span>。把<span
class="math inline">\(O_i\)</span>（该<span
class="math inline">\(O_i\)</span>为第一次计算而得）展开后为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，展开的<span
class="math inline">\(O_i\)</span>乘以<span class="math inline">\(e^{m_i
- m_{i}^{new}}\)</span>，等于<span class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，<span
class="math inline">\(m_i\)</span>就是<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，所以相互抵消了。注意，此处的<span
class="math inline">\(S_{ij}\)</span>是第一次计算得到的。红框最后结果<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>就是当前位置之前的V的加权和。绿框部分把<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>展开，为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}\)</span>，绿框最后结果为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，此处的<span
class="math inline">\(S_{ij}\)</span>是第二次计算得到的。最后除以最外层的<span
class="math inline">\(\ell_{i}^{new}\)</span>，也就是softmax的分母。往后的计算也是同样的逻辑，之前算出的<span
class="math inline">\(O_i\)</span>加上当前块计算得到的绿框部分。当遍历完一行时，<span
class="math inline">\(O_i\)</span>这一行也就计算完了。
![[file-20250712112231090.png]]</p>
<p>第十三步，更新<span class="math inline">\(\ell_i\)</span>和<span
class="math inline">\(m_i\)</span>，更新后的值写回HBM。返回执行第五和第七步的循环，求得完整的O。</p>
<h2 id="recomputation">3.2 Recomputation</h2>
<p>原始的attention在反向传播时，通过S矩阵和P矩阵更新梯度，<span
class="math inline">\(S = QK^T\)</span>，<span class="math inline">\(P =
softmax(S)\)</span>，两个矩阵各自都需要<span
class="math inline">\(N^2\)</span>的空间存储。论文中通过最终结果O，以及之前求的softmax统计量m和<span
class="math inline">\(\ell\)</span>，在SRAM中重新计算S和P来进行梯度更新。虽然相比原始attention，计算量增加了，但是空间复杂度降低为N（O的shape为（N，
d）），IO时间大大减少，所以重计算能比原始attention节省时间。</p>
]]></content>
  </entry>
  <entry>
    <title>走进大模型（四）——flash-attention</title>
    <url>/2025/07/20/%E8%B5%B0%E8%BF%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B(%E5%9B%9B)/</url>
    <content><![CDATA[<p>self-attention的计算耗时，同时也需要大量空间，假设输入长度为N，那么空间复杂度和时间复杂度都是<span
class="math inline">\(N^2\)</span>，所以早期模型的上下文长度一般很有限。而flash-attention的优化，则能把原始attention的空间复杂度缩小为N。本文将细致地讲解flash-attention的实现，做到通俗易懂，对于性能优化零基础的人也能看懂。flash-attention论文链接：<a
href="https://arxiv.org/abs/2205.14135">[2205.14135] FlashAttention:
Fast and Memory-Efficient Exact Attention with IO-Awareness</a>。
<span id="more"></span></p>
<h1 id="总览">1.总览</h1>
现代计算机基于冯诺依曼架构，如图所示，分为五部分：输入设备、输出设备、存储单元、控制单元、运算单元。
<div>
<p><img src="\images\llm4\file-20250708212909164.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
完成计算需要先从储存中读出数据，调用计算核心进行计算，再将计算结果写回存储。所以在做性能优化时。往往从两方面考虑，一个是读写数据角度，一个是计算角度。有的计算可能在计算方面耗时多，有的计算可能在读写时耗时，也有的两者兼有。通过屋脊模型，可以量化地衡量某个计算是compute-bound还是memory-bound，关于屋脊模型的讲解，后续我会写一篇文章。对于flash-attention来讲，在数据读写方面很耗时，所以作者的核心思想就是通过减少数据的读写次数来加速计算。具体表现为两点：1.将矩阵分块，在SRAM（Static
Random-Access Memory）中完成计算，再写回HBM（High Bandwidth
Memory）。2.不存储中间结果，在反向传播时，通过重新计算中间结果来更新梯度。从下图（来源于论文）可以看到，flash-attention相比普通的attention，速度快了不少。
<div>
<p><img src="\images\llm4\file-20250708224435562.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<h1 id="原始attention">2.原始attention</h1>
我们先来看下原始的attention计算过程，Q，K，V都存在HBM中。三者shape都为（N，
d）N是输入序列的长度，d是注意力头的维度，一般来说，N远大于d，GPT2中，N为2014，d为64。第一步，从HBM中加载Q，K，计算得到注意力矩阵S，把S写回HBM，再从HBM中读入S，求softmax，得到P，再把P写回HBM，最后从HBM中加载P，V，计算得到最终结果O，写回HBM。
<div>
<p><img src="\images\llm4\file-20250708221951972.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
HBM相比SRAM来说，存储容量大，读写速度慢。对于储存来说，读写速度一般与其容量呈反比，受限于成本，读写速度快的，容量就小。因此数据反复从HBM读写很耗时。下图反映了不同储存在读写速度和容量方面的差异。
<div>
<p><img src="\images\llm4\file-20250708223339056.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<h1 id="flash-attention">3.flash-attention</h1>
flash-attention的优化分为两个方面，一个是分块计算，一个是更新梯度时重计算中间结果，重新计算中间结果的目的是为了不保存空间复杂度为<span
class="math inline">\(N^2\)</span>的中间结果，减少IO时间。 ## 3.1 Tiling
我们先看论文里作者给出的flash-attention的分块计算过程，分别对Q，K，V进行分块后，从HBM读出数据到SRAM，计算完成后，在把数据写入HBM。依次对每一个分块进行处理，直到计算完所有分块。
<div>
<p><img src="\images\llm4\file-20250708224029546.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
论文给出了以上计算过程的详细算法，如下所示，我会对每一步进行讲解。
<div>
<p><img src="\images\llm4\file-20250708224719815.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>第一步，计算分块的行数<span
class="math inline">\(B_c\)</span>和列数<span
class="math inline">\(B_r\)</span>，M是SRAM的大小，<span
class="math inline">\(B_c\)</span>等于<span
class="math inline">\(\frac{M}{4\mathrm{d}}\)</span>，除4d是要保证SRAM能够装下4个分块，分别是Q，K，V，以及最终结果O。<span
class="math inline">\(B_r\)</span>在<span
class="math inline">\(\frac{M}{4\mathrm{d}}\)</span>和d之间取最小值是为了保证SRAM能够装下中间计算用到的所有数据，除了QKVO的分块，还有<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>，<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>等。</p>
第二步会初始化O，<span
class="math inline">\({\ell}\)</span>，m，分别为0，0，负无穷。<span
class="math inline">\({\ell}\)</span>，m是在计算softmax过程中用到的两个统计值，分别代表局部的指数和以及局部最大值。我们回顾一下softmax的计算方法，下图中x为输入向量，<span
class="math inline">\(m(x)\)</span>为x所有值中的最大值，<span
class="math inline">\(f(x)\)</span>中的每一项，指数部分减去<span
class="math inline">\(m(x)\)</span>，是为了保持计算时数值稳定。<span
class="math inline">\({\ell(x)}\)</span>是<span
class="math inline">\(f(x)\)</span>的累加和，即softmax的分母。
<div>
<p><img src="\images\llm4\file-20250712090217340.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
由原始的softmax计算可知，得到一个向量的softmax值，需要完整的向量，因为分母求和会用到向量的每一个元素，并且分子指数部分减去最大值，最大值也需要遍历完整向量才能获取。论文对QK分块后，是怎么计算softmax的呢？我们看下图：
<div>
<p><img src="\images\llm4\file-20250712101937966.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>假设向量x被分为了2块，分别为<span
class="math inline">\(x^\text{(1)}\)</span>，<span
class="math inline">\(x^\text{(2)}\)</span>，首先根据公式一计算<span
class="math inline">\(m(x^\text{(1)})\)</span>，<span
class="math inline">\(f(x^\text{(1)})\)</span>和<span
class="math inline">\({\ell(x^\text{(1)})}\)</span>，接下来读取<span
class="math inline">\(x^\text{(2)}\)</span>，计算<span
class="math inline">\(m(x)\)</span>，即<span
class="math inline">\(x^\text{(1)}\)</span>，<span
class="math inline">\(x^\text{(2)}\)</span>的最大值，<span
class="math inline">\(m(x)\)</span>就是向量x的最大值，接着就是求softmax的分子部分——<span
class="math inline">\(f(x)\)</span>，<span
class="math inline">\(e^{m(x^{(1)}) -
m(x)}f(x^{(1)})\)</span>怎么理解，其实就是原始softmax的分子部分，向量<span
class="math inline">\(x^{(1)}\)</span>的每个元素减去向量x中的最大值。把<span
class="math inline">\(f(x^{(1)}\)</span>展开后，等于<span
class="math inline">\([e^{x_1 - m(x^{(1)})}···e^{x_B -
m(x^{(1)})}]\)</span>。<span class="math inline">\(e^{m(x^{(1)}) -
m(x)}\)</span>和<span
class="math inline">\(f(x^{(1)}\)</span>相乘，指数部分的<span
class="math inline">\(m(x^{(1)})\)</span>就没了。<span
class="math inline">\(e^{m(x^{(2)}) -
m(x)}f(x^{(2)})\)</span>也是同理，我们就得到了计算softmax所需的分子。分母部分<span
class="math inline">\(e^{m(x^{(1)}) -
m(x)}\ell(x^{(1)})\)</span>也是一样的，把<span
class="math inline">\(\ell(x^{(1)})\)</span>展开，指数部分的<span
class="math inline">\(m(x^{(1)})\)</span>就被消除了。由此得到分母<span
class="math inline">\(\ell(x)\)</span>，从而计算出向量x的softmax。分块计算的结果完全等价于对完整的x向量进行softmax计算。</p>
<p>下面以一个具体的例子来看公式二的计算，公式二是理解flash-attention的关键。算法的第十一步和第十二步都是基于公式二进行。
我们有两个块：</p>
<p><strong>Block1</strong>:<br />
<span class="math inline">\(x^{(1)} = [2.0,\ 3.0]\)</span></p>
<p><strong>Block2</strong>:<br />
<span class="math inline">\(x^{(2)} = [0.0,\ 4.0]\)</span></p>
<p><strong>拼接后的向量为</strong>：<br />
<span class="math inline">\(x = [2.0,\ 3.0,\ 0.0,\ 4.0]\)</span></p>
<p><strong>块内局部最大值</strong> <span class="math display">\[
\begin{aligned}
m^{(1)} &amp;= \max(2.0,\ 3.0) = 3.0 \\
m^{(2)} &amp;= \max(0.0,\ 4.0) = 4.0 \\
m(x) &amp;= \max(m^{(1)},\ m^{(2)}) = \max(3.0,\ 4.0) = 4.0
\end{aligned}
\]</span><strong>各块的 <span class="math inline">\(f(x^{(i)})\)</span>
和 <span class="math inline">\(\ell(x^{(i)})\)</span></strong> 对于
Block1： <span class="math display">\[
\begin{aligned}
f(x^{(1)}) = \exp(x^{(1)} - m^{(1)}) &amp;= [e^{2.0 - 3.0},\ e^{3.0 -
3.0}] = [e^{-1},\ e^0] = [0.3679,\ 1] \\
\ell(x^{(1)}) &amp;= 0.3679 + 1 = 1.3679 \\
f(x^{(\text{block}_1)}) &amp;= e^{m^{(1)} - m(x)} \cdot f(x^{(1)}) \\
&amp;= e^{m^{(1)} - m(x)} \cdot \exp(x^{(1)} - m^{(1)}) \\
&amp;= e^{-1} \cdot [0.3679,\ 1] = [0.1353,\ 0.3679]
\end{aligned}
\]</span></p>
<p>对于 Block2： <span class="math display">\[
\begin{aligned}
f(x^{(2)}) = \exp(x^{(2)} - m^{(2)}) &amp;= [e^{0.0 - 4.0},\ e^{4.0 -
4.0}] = [e^{-4},\ 1] = [0.0183,\ 1] \\
\ell(x^{(2)}) &amp;= 0.0183 + 1 = 1.0183 \\
f(x^{(\text{block}_2)}) &amp;= e^{m^{(2)} - m(x)} \cdot f(x^{(2)}) \\
&amp;= e^{m^{(2)} - m(x)} \cdot \exp(x^{(2)} - m^{(2)}) \\
&amp;= e^0 \cdot [0.0183,\ 1] = [0.0183,\ 1]
\end{aligned}
\]</span> <strong>合并 <span class="math inline">\(\ell(x)\)</span> 和
<span class="math inline">\(f(x)\)</span></strong> <span
class="math display">\[
\begin{aligned}
\ell(x) &amp;= e^{m^{(1)} - m(x)} \cdot \ell(x^{(1)}) + e^{m^{(2)} -
m(x)} \cdot \ell(x^{(2)}) \\
&amp;= e^{-1} \cdot 1.3679 + e^0 \cdot 1.0183 \\
&amp;= 0.3679 \cdot 1.3679 + 1.0183 = 0.5034 + 1.0183 = 1.5217
\end{aligned}
\]</span></p>
<p><span class="math display">\[
f(x) = \begin{bmatrix}
f(x^{(block_1)}) \\
f(x^{(block_2)})
\end{bmatrix}
= [0.1353,\ 0.3679,\ 0.0183,\ 1.0]
\]</span></p>
<p><strong>Softmax输出</strong></p>
<p>我们使用上面得到的 <span class="math inline">\(\ell(x)\)</span> 和
<span class="math inline">\(f(x)\)</span> 计算softmax：</p>
<p><span class="math display">\[
\text{softmax}(x) = \frac{f(x)}{\ell(x)}
\]</span></p>
<p><span class="math display">\[
\text{softmax}(x) \approx [0.0889,\ 0.2417,\ 0.0120,\ 0.6569]
\]</span></p>
<p>第三步把Q按行分为<span class="math inline">\(T_r\)</span>个块，<span
class="math inline">\(T_r\)</span>的计算方法在图中已给出，每个块shape为（<span
class="math inline">\(B_r\)</span>， d），K和V也是按行分块，分为<span
class="math inline">\(T_c\)</span>个块，<span
class="math inline">\(T_c\)</span>的计算同样在图中已给出，每个块shape为（<span
class="math inline">\(B_c\)</span>， d）。第四步也是分块，O，<span
class="math inline">\({\ell}\)</span>，m，都分为<span
class="math inline">\(T_r\)</span>个块。</p>
<p>第五步开始就是计算的主体部分，第五第六步按列将分块的K和V从HBM加载到SRAM。第七第八步将分块的Q，O以及两个统计值<span
class="math inline">\(\ell\)</span>和m按行从HBM加载到SRAM。</p>
<p>第九步，在SRAM中计算注意力矩阵——<span
class="math inline">\(S_{ij}\)</span>，<span
class="math inline">\(S_{ij}\)</span>shape为（<span
class="math inline">\(B_r\)</span>，<span
class="math inline">\(B_c\)</span>）。</p>
第十步，在SRAM中计算<span
class="math inline">\(S_{ij}\)</span>每一行的最大值<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，softmax的分子<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>，softmax的分母<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>。
<div>
<p><img src="\images\llm4\file-20250712110454677.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
第十一步，也是在SRAM中计算，在下方附上公式方便查阅。这一步是计算根据读入的分块，计算新的最大值<span
class="math inline">\(m_{i}^{new}\)</span>，新的指数和<span
class="math inline">\(\ell_{i}^{new}\)</span>。进行第一次计算时，<span
class="math inline">\(m_i\)</span>是负无穷（初始化时的值），所以<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，因为<span
class="math inline">\(\ell_i\)</span>初始化为0，所以<span
class="math inline">\(\ell_{i}^{new}\)</span>计算公式的第一部分为0，第二部分指数部分为0（因为<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>），<span
class="math inline">\(\ell_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{\ell}_{\text{i,j}}\)</span>。往后每一次的更新逻辑，其实和上文提到的公式二是一样的，<span
class="math inline">\(m_{i}^{new}\)</span>是从开头到当前块的最大值，<span
class="math inline">\(\ell_{i}^{new}\)</span>是从开头到当前块的指数和。
<div>
<p><img src="\images\llm4\file-20250712105924118.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
第十二步，在SRAM中计算出<span
class="math inline">\(O_i\)</span>后，把<span
class="math inline">\(O_i\)</span>写回HBM，下方附上公式。<span
class="math inline">\(O_i\)</span>是<span
class="math inline">\(\text{softmax}\left( QK^\top \right)
V\)</span>。<span
class="math inline">\(diag{(\ell_{i}^{new})}^{-1}\)</span>表示将<span
class="math inline">\(\ell_{i}^{new}\)</span>向量转换为对角矩阵（只有对角线位置有值，其余位置为0），指数-1表示取倒数。<span
class="math inline">\(diag{(\ell_{i}^{new})}^{-1}\)</span>其实就是做除法。看第一部分红框，第一次计算时，因为<span
class="math inline">\(\ell_i\)</span>
为0，所以红框部分为0，绿框部分<span
class="math inline">\(e^{\tilde{m}_{\text{i,j}} -
m_{i}^{new}}\tilde{p}_{\text{i,j}}V_j\)</span>，第一次计算时<span
class="math inline">\(m_{i}^{new}\)</span>等于<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，所以只剩<span
class="math inline">\(\tilde{p}_{\text{i,j}}V_j\)</span>，<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>是softmax的分子部分。乘<span
class="math inline">\(V_j\)</span>，除<span
class="math inline">\(\ell_{i}^{new}\)</span>，得到<span
class="math inline">\(O_i\)</span>的部分结果。第二次计算时，红框部分乘<span
class="math inline">\(diag(\ell_i)\)</span>是为了抵消第一次除<span
class="math inline">\(\ell_{i}^{new}\)</span>，因为在每次计算都是在最外层除<span
class="math inline">\(\ell_{i}^{new}\)</span>，为了避免多除，所以要乘<span
class="math inline">\(diag(\ell_i)\)</span>。把<span
class="math inline">\(O_i\)</span>（该<span
class="math inline">\(O_i\)</span>为第一次计算而得）展开后为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，展开的<span
class="math inline">\(O_i\)</span>乘以<span class="math inline">\(e^{m_i
- m_{i}^{new}}\)</span>，等于<span class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，<span
class="math inline">\(m_i\)</span>就是<span
class="math inline">\(\tilde{m}_{\text{i,j}}\)</span>，所以相互抵消了。注意，此处的<span
class="math inline">\(S_{ij}\)</span>是第一次计算得到的。红框最后结果<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>就是当前位置之前的V的加权和。绿框部分把<span
class="math inline">\(\tilde{p}_{\text{i,j}}\)</span>展开，为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}\)</span>，绿框最后结果为<span
class="math inline">\(e^{S_{ij} -
\tilde{m}_{\text{i,j}}}V_j\)</span>，此处的<span
class="math inline">\(S_{ij}\)</span>是第二次计算得到的。最后除以最外层的<span
class="math inline">\(\ell_{i}^{new}\)</span>，也就是softmax的分母。往后的计算也是同样的逻辑，之前算出的<span
class="math inline">\(O_i\)</span>加上当前块计算得到的绿框部分。当遍历完一行时，<span
class="math inline">\(O_i\)</span>这一行也就计算完了。
<div>
<p><img src="\images\llm4\file-20250712112231090.png" style="max-width: 50%; max-height: 50%;"></p>
</div>
<p>第十三步，更新<span class="math inline">\(\ell_i\)</span>和<span
class="math inline">\(m_i\)</span>，更新后的值写回HBM。返回执行第五和第七步的循环，求得完整的O。</p>
<h2 id="recomputation">3.2 Recomputation</h2>
<p>原始的attention在反向传播时，通过S矩阵和P矩阵更新梯度，<span
class="math inline">\(S = QK^T\)</span>，<span class="math inline">\(P =
softmax(S)\)</span>，两个矩阵各自都需要<span
class="math inline">\(N^2\)</span>的空间存储。论文中通过最终结果O，以及之前求的softmax统计量m和<span
class="math inline">\(\ell\)</span>，在SRAM中重新计算S和P来进行梯度更新。虽然相比原始attention，计算量增加了，但是空间复杂度降低为N（O的shape为（N，
d）），IO时间大大减少，所以重计算能比原始attention节省时间。</p>
]]></content>
      <categories>
        <category>llm技术</category>
      </categories>
      <tags>
        <tag>大模型</tag>
        <tag>llm</tag>
        <tag>flash-attention</tag>
      </tags>
  </entry>
</search>
